---
chapter: 데이터 분석
title: 간단한 Pandas
date: 2024-09-24
---

# 1. Pandas

Pandas는 데이터 분석을 위한 라이브러리로, Numpy와 함께 데이터 분석에 있어서 가장 기본이 되는 라이브러리입니다. Pandas는 데이터를 효과적으로 처리할 수 있는 다양한 기능을 제공합니다. 이번 챕터에서는 Pandas의 기본적인 사용법을 알아보겠습니다.

## 1.1 Pandas의 데이터 구조

Pandas는 크게 Series와 DataFrame 두 가지 데이터 구조를 제공합니다. Series는 1차원 배열이고, DataFrame은 2차원 배열입니다. Series와 DataFrame은 Numpy 배열과 유사하지만, 인덱스를 가지고 있어 데이터를 더 효율적으로 처리할 수 있습니다.

::img{src="/images/essentials-numpy-pandas/chapter04/pandas_data_structure.png"}

## 1.2 Pandas Series

Pandas Series는 1차원 배열입니다. Series는 인덱스를 가지고 있어 데이터를 더 효율적으로 처리할 수 있습니다. 이 인덱스는 데이터의 레이블 역할을 하며, 데이터를 빠르게 검색할 수 있습니다.

```python-exec
import pandas as pd

data = [1, 2, 3, 4, 5]
s = pd.Series(data)
print(s)
print(s[0])
print(s[1:4])
```

아래는 Series의 인덱스를 지정하는 예제입니다.

```python-exec
import pandas as pd

data = [1, 2, 3, 4, 5]
index = ['a', 'b', 'c', 'd', 'e']
s = pd.Series(data, index=index)
print(s)
print(s['a'])
print(s['b':'d'])
# print(s[0]) # 이 방식도 가능합니다.
```

인덱스를 지정하지 않으면 0부터 시작하는 정수 인덱스가 자동으로 생성됩니다. 지정을 하면 해당 인덱스로 데이터를 조회할 수 있습니다. 이렇게 지정되지 않았을 때 0부터 시작하는 정수 인덱스를 **묵시적 인덱스**라고 합니다. 또한 인덱스를 지정하여 데이터를 조회할 때에는 **명시적 인덱스**라고 합니다.

시리즈와 스칼라는 아래와 같이 연산 됩니다.

```python-exec
import pandas as pd

data = [1, 2, 3, 4, 5]
s = pd.Series(data)
print(s + 10)
print(s * 10)
```

시리즈와 시리즈는 아래와 같이 연산 됩니다. 이번에는 인덱스를 다르게 설정해보겠습니다.

```python-exec
import pandas as pd

data1 = [1, 2, 3, 4, 5]
data2 = [10, 20, 30, 40, 50]
index1 = ['a', 'b', 'c', 'd', 'e']
index2 = ['a', 'c', 'b', 'e', 'g']

s1 = pd.Series(data1, index=index1)
s2 = pd.Series(data2, index=index2)
print(s1 + s2)
```

인덱스에 맞는 값끼리 연산된 것을 확인할 수 있습니다. 두 시리즈의 인덱스가 일치하지 않는 경우에는 NaN(Not a Number)으로 표시됩니다.

이번에는 시리즈와 배열을 연산해보겠습니다. 이때는 시리즈의 인덱스가 무시되고, 배열의 인덱스가 사용됩니다. 연산된 데이터 타입은 시리즈가 됩니다.

```python-exec
import pandas as pd

data = [1, 2, 3, 4, 5]
s = pd.Series(data)
arr = [10, 20, 30, 40, 50]
print(s + arr)
```

## 1.3 Pandas DataFrame

Pandas DataFrame은 2차원 배열입니다. DataFrame은 행과 열로 구성되어 있으며, 각 열은 서로 다른 데이터 타입을 가질 수 있습니다. DataFrame은 여러 개의 Series로 구성되어 있습니다.

```python-exec
import pandas as pd

data = {
    'name': ['John', 'Anna', 'Peter', 'Linda'],
    'age': [25, 36, 29, 24],
    'city': ['New York', 'Paris', 'Berlin', 'London']
}
df = pd.DataFrame(data)
df
```

DataFrame은 딕셔너리 형태로 데이터를 입력할 수 있습니다. 딕셔너리의 키는 열 이름이 되고, 값은 해당 열의 데이터가 됩니다. DataFrame은 열 이름을 통해 데이터를 조회할 수 있습니다.

앞서 배운 시리즈를 조합하여 데이터프레임을 만들 수도 있습니다. 데이터프레임에 각각의 컬럼은 실제로 시리즈로 구성되어 있다는 것을 기억해주세요.

```python-exec
import pandas as pd

s1 = pd.Series(['John', 'Anna', 'Peter', 'Linda'])
s2 = pd.Series([25, 36, 29, 24])
s3 = pd.Series(['New York', 'Paris', 'Berlin', 'London'])
df = pd.DataFrame({'name': s1, 'age': s2, 'city': s3})

df
```

## 1.4 데이터 불러오기

:::figure
::img{src="https://pandas.pydata.org/docs/_images/02_io_readwrite.svg"}
::figcaption[출처: Pandas 공식 문서]
:::

Pandas는 다양한 데이터 포맷을 지원합니다. CSV, Excel, SQL, JSON 등 다양한 데이터를 불러올 수 있습니다. 앞서 타이타닉 예제는 CSV 파일을 불러온 것이고, 이번에는 HTML을 불러오도록 하겠습니다. 불러올 페이지는 아래와 같습니다.

::a[크롤링 연습 페이지]{class='btn-link' href="https://paullab.co.kr/stock.html" target="\_blank"}

```python-exec
import pandas as pd
from pyodide.http import open_url

data = open_url('https://paullab.co.kr/stock.html')
df = pd.read_html(data)
df[0] # 0, 1, 2 등을 차례로 넣어보세요.
```

위니북스에서만 `open_url` 함수를 사용하고 colab에서는 더 간단한 코드로 작성할 수 있습니다. 아래 코드는 colab에서 사용하는 코드입니다.

```python
import pandas as pd

df = pd.read_html('https://paullab.co.kr/stock.html')
df[0] # 0, 1, 2 등을 차례로 넣어보세요.
```

여기에서 준비한 데이터가 아니고 웹에 있는 데이터라면 아래처럼 불러와서 실행할 수 있습니다. 위키백과에서 대한민국의 인구 데이터를 불러오는 예제입니다. `https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC`는 `https://ko.wikipedia.org/wiki/대한민국의_인구`입니다. 한글을 넣으면 애러가 발생하여 URL 인코딩이 필요합니다.

```python
import pandas as pd

df = pd.read_html('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC')
df # df[0], df[1], df[2] 등을 차례로 넣어보세요.
# df[0]['2020']
# df[0]['2020'].to_csv('test.csv')
# df[0]['2020'].to_json('test2.json')
# df[0]['2020'].to_html('test3.html')
# df = pd.read_html('test3.html')
# df
```

colab에서 함께 실습할만한 코드를 몇 개 더 추가하였습니다.

## 1.5 데이터 저장하기

Pandas는 데이터를 다양한 형식으로 저장할 수 있습니다. CSV, Excel, SQL, JSON 등 다양한 형식으로 저장할 수 있습니다. 아래는 DataFrame을 CSV 파일로 저장하는 예제입니다. 위니북스에서는 파일 저장 기능이 제한되어 있어, 해당 코드를 실행하고 싶다면 colab에서 실행해주세요.

```python
import pandas as pd

data = {
    'name': ['John', 'Anna', 'Peter', 'Linda'],
    'age': [25, 36, 29, 24],
    'city': ['New York', 'Paris', 'Berlin', 'London']
}
df = pd.DataFrame(data)
df.to_csv('data.csv', index=False)
```

위 코드를 실행하면, 현재 디렉토리에 `data.csv` 파일이 생성됩니다. `index=False`는 인덱스를 저장하지 않도록 하는 옵션입니다.

## 1.6 인덱싱과 슬라이싱

Pandas는 Numpy와 유사하게 인덱싱과 슬라이싱을 지원합니다. 인덱싱은 열 이름을 사용하거나, `loc`, `iloc`을 사용하여 데이터를 조회할 수 있습니다.

```python-exec
import pandas as pd

data = {
    'name': ['John', 'Anna', 'Peter', 'Linda'],
    'age': [25, 36, 29, 24],
    'city': ['New York', 'Paris', 'Berlin', 'London']
}
df = pd.DataFrame(data)
df
```

:::figure
::img{src="https://pandas.pydata.org/docs/_images/03_subset_columns.svg"}
::figcaption[출처: Pandas 공식 문서]
:::

열을 선택하는 방법은 아래와 같습니다. `df['name']`와 같이 하나의 데이터를 선택하거나 `df[['name', 'age']]`와 같이 2개 이상의 데이터를 선택할 수 있습니다.

```python-exec
df['name']
df[['name', 'age']]
# df[['age', 'height']].median() # 이렇게도 사용할 수 있습니다.
```

:::figure
::img{src="https://pandas.pydata.org/docs/_images/03_subset_rows.svg"}
::figcaption[출처: Pandas 공식 문서]
:::

행을 선택하는 방법은 아래와 같습니다. `df.loc[0]`와 `df.iloc[0]`는 같은 결과를 반환합니다. 다만 `loc`는 사용자가 입력한 명시적인 인덱스를 사용하고, `iloc`는 자동으로 할당된 묵시적 인덱스를 사용합니다. 사용자가 입력한 인덱스가 없을 경우 명시적 인덱스도 0부터 시작하는 정수 인덱스와 같은 결과를 반환합니다.

```python-exec
print(df.loc[0])
# print(df.iloc[0])
# print(df.iloc[0:2])
```

데이터는 행과 열에서 모두 선택하실 수 있습니다.

```python-exec
df.loc[0]['city']
# df.loc[0, 'name'] # 이렇게도 가능합니다.
# df['city'][0] # 반대로도 가능합니다.
# df.loc[0:2, 'name']
```

## 1.7 데이터 조작

Pandas는 데이터를 조작하고 변환하는 다양한 기능을 제공합니다. 가장 기본적인 데이터 조작 방법들을 살펴보겠습니다.

### 1.7.1 컬럼 조작

컬럼을 생성하고 삭제하는 방법을 살펴보겠습니다.

```python-exec
import pandas as pd

# 샘플 데이터 생성
data = {
    'name': ['John', 'Anna', 'Peter', 'Linda'],
    'age': [25, 36, 29, 24],
    'city': ['New York', 'Paris', 'Berlin', 'London']
}
df = pd.DataFrame(data)
df
```

새로운 컬럼을 추가하거나, 기존 컬럼의 값을 수정할 수 있습니다.

```python-exec
# 새로운 컬럼 추가
df['age_group'] = ['20대', '30대', '20대', '20대']

# 컬럼 값 수정
df['age'] = df['age'] + 1

# 여러 컬럼을 사용한 새로운 컬럼 생성
df['info'] = df['name'] + ' from ' + df['city']

df
```

drop을 사용하면 컬럼을 삭제할 수 있습니다.

```python-exec
# 컬럼 삭제
df = df.drop('info', axis=1)
df
```

### 1.7.2 결측치 처리

결측치는 데이터에 값이 없는 경우를 의미합니다. 결측치는 데이터 분석에 있어서 문제가 될 수 있으므로, 적절한 방법으로 처리해야 합니다. Pandas는 결측치를 확인하고 제거하거나 채우는 다양한 기능을 제공합니다.

```python-exec
import pandas as pd

# 결측치가 있는 데이터 생성
data = {
    'name': ['John', 'Anna', None, 'Linda'],
    'age': [25, None, 29, 24],
    'city': ['New York', 'Paris', 'Berlin', None]
}
df = pd.DataFrame(data)

# 결측치 확인
print("결측치 확인:")
print(df.isnull().sum())
df
```

결측치를 제거하거나 채울 수 있습니다. 이 때 원본을 변경하려면 `df.dropna(inplace=True)`를 사용하면 됩니다. 여기서는 원본을 변경하지 않고 결과만 출력하도록 하겠습니다.

```python-exec
# 결측치 제거
print("결측치 제거:")
df.dropna()
```

결측치를 채우는 방법은 다양합니다. 평균값, 중앙값, 최빈값 등으로 채울 수 있습니다. 여기서는 평균값으로 채우는 예제를 살펴보겠습니다.

```python-exec
# 결측치 채우기
print("결측치 채우기:")
df.fillna({'name': 'Unknown', 'age': df['age'].mean(), 'city': 'Unknown'})
```

이렇게 결측치를 처리하지 않고, 아래와 같이 단일 값으로 채울 수도 있습니다. 다만 이렇게 하면 모든 컬럼에 동일한 값이 채워지므로 주의해야 합니다. `age` 컬럼의 경우 숫자형임에도 불구하고 문자열로 채워지는 것을 확인할 수 있습니다.

```python-exec
# 단일 값으로 결측치 채우기
print("단일 값으로 결측치 채우기:")
df.fillna('Unknown')
```

## 1.8 데이터 필터링

데이터 필터링은 조건에 맞는 데이터만 추출하는 과정입니다. Pandas에서는 다양한 방식으로 데이터를 필터링할 수 있습니다.

### 1.8.1 Boolean 인덱싱

Numpy에서 배운 불리언 인덱싱과 유사하게 Pandas에서도 불리언 인덱싱을 사용할 수 있습니다. 불리언 인덱싱은 조건을 만족하는 데이터만 선택할 때 사용합니다.

```python-exec
import pandas as pd

# 샘플 데이터 생성
data = {
    'name': ['John', 'Anna', 'Peter', 'Linda', 'Tom'],
    'age': [25, 36, 29, 24, 32],
    'city': ['New York', 'Paris', 'Berlin', 'London', 'Paris'],
    'salary': [50000, 60000, 45000, 55000, 65000]
}
df = pd.DataFrame(data)
df
```

```python-exec
# 단순 조건
print("30세 이상인 사람:")
df[df['age'] >= 30]
```

```python-exec
# 복합 조건 (AND)
print("30세 이상이고 Paris에 사는 사람:")
df[(df['age'] >= 30) & (df['city'] == 'Paris')]
```

```python-exec
# 복합 조건 (OR)
print("Paris나 London에 사는 사람:")
df[(df['city'] == 'London') | (df['city'] == 'Paris')]
```

### 1.8.2 query 함수 사용

query 함수를 사용하면 SQL 쿼리처럼 데이터를 필터링할 수 있습니다.

```python-exec
# query 함수를 사용한 필터링
print("salary가 50000 이상인 사람:")
df.query('salary >= 50000')
```

```python-exec
print("Paris에 살고 30세 이상인 사람:")
df.query('city == "Paris" and age >= 30')
```

조금 더 복잡한 조건을 사용해보도록 하겠습니다. 이렇게 복잡한 조건은 query 함수를 사용하는 것이 더 편리합니다.

```python-exec
import pandas as pd

# 더 많은 정보를 가진 샘플 데이터 생성
data = {
    'name': ['John', 'Anna', 'Peter', 'Linda', 'Tom', 'Lisa', 'Sarah', 'Mike'],
    'department': ['IT', 'HR', 'IT', 'HR', 'IT', 'Marketing', 'Marketing', 'HR'],
    'age': [25, 36, 29, 24, 32, 27, 31, 28],
    'salary': [50000, 60000, 45000, 55000, 65000, 52000, 58000, 51000],
    'experience': [2, 8, 4, 3, 6, 4, 5, 3]
}
df = pd.DataFrame(data)
df
```

```python-exec
# 복잡한 조건 조합
print("(IT 부서이면서 급여 6만 이상) 또는 (HR 부서이면서 경력 5년 이상)인 직원:")
df.query('(department == "IT" and salary >= 60000) or (department == "HR" and experience >= 5)')
```

## 1.9 데이터 그룹화와 집계

그룹화는 데이터를 특정 기준에 따라 묶어서 분석하는 것입니다. groupby 함수를 사용하여 데이터를 그룹화할 수 있습니다.

```python-exec
import pandas as pd

# 샘플 데이터 생성
data = {
    'name': ['John', 'Anna', 'Peter', 'Linda', 'Tom', 'Lisa'],
    'department': ['IT', 'HR', 'IT', 'HR', 'IT', 'HR'],
    'age': [25, 36, 29, 24, 32, 27],
    'salary': [50000, 60000, 45000, 55000, 65000, 52000]
}
df = pd.DataFrame(data)
df
```

아래 코드처럼 groupby 함수를 사용하여 데이터를 그룹화할 수 있습니다.

```python-exec
# 부서별 인원수
print("부서별 인원수:")
df.groupby('department').size()
```

아래와 같이 여러 컬럼을 기준으로 그룹화할 수도 있습니다.

```python-exec
# 부서별 평균 연봉
print("부서별 평균 연봉:")
df.groupby('department')['salary'].mean()
```

apply 함수를 사용하여 그룹화된 데이터에 대해 집계 함수를 적용할 수 있습니다.

```python-exec
# 부서별 최대 연봉
print("부서별 최대 연봉:")
df.groupby('department')['salary'].apply(max)
```

여기서 사용된 apply 함수는 데이터를 변환할 때 사용하는 함수입니다. 예를 들어 아래와 같이 간단한 함수를 만들어서 사용할 수 있습니다.

```python-exec
data = {
    'name': ['Licat', 'John', 'Anna', 'Peter', 'Linda', 'Tom', 'Lisa'],
    'age': [17, 18, 15, 29, 24, 32, 27]
}
df_apply = pd.DataFrame(data)
df_apply
```

```python-exec
def age_plus(age):
    return age + 10

df_apply['age_plus'] = df_apply['age'].apply(age_plus)
# df_apply['age'].apply(lambda x: x + 10)
df_apply
```

아래와 같이 사용할 수도 있습니다.

```python-exec
def age_filter(age):
    if age >= 20:
        return '성인'
    else:
        return '미성년'

df_apply['구분'] = df_apply['age'].apply(age_filter)
df_apply
```

다시 groupby로 돌아와서 만약 groupby 함수를 사용하지 않으면, 아래와 같이 직접 데이터를 하나씩 필터링하여 집계 함수를 적용해야 합니다.

```python-exec
df[df['department'] == 'IT']['age'].mean()
df[df['department'] == 'HR']['age'].mean()
```

## 1.10 집계 함수

Pandas는 데이터를 분석하기 위한 다양한 집계 함수를 제공합니다.

```python-exec
import pandas as pd

# 샘플 데이터 계속 사용
print("기본 통계량:")
print(df['salary'].describe())

print("\n각종 집계 함수:")
print("평균값:", df['salary'].mean())
print("중앙값:", df['salary'].median())
print("최대값:", df['salary'].max())
print("최소값:", df['salary'].min())
print("표준편차:", df['salary'].std())
print("합계:", df['salary'].sum())

# 여러 컬럼에 대한 집계
print("\n여러 컬럼 집계:")
df[['age', 'salary']].agg(['mean', 'median', 'std'])
```

## 1.11 데이터 사전 분석

데이터 분석을 시작하기 전에 데이터의 전반적인 특성을 파악하는 것이 중요합니다. Pandas는 이를 위한 다양한 함수를 제공합니다.

```python-exec
import pandas as pd

# 샘플 데이터 생성 (조금 더 큰 데이터셋)
data = {
    'name': ['John', 'Anna', 'Peter', 'Linda', 'Tom', 'Lisa', 'Sarah', 'Mike', 'David', 'Emma'],
    'department': ['IT', 'HR', 'IT', 'HR', 'IT', 'HR', 'IT', 'HR', 'IT', 'HR'],
    'age': [25, 36, 29, 24, 32, 27, 31, 28, 35, 26],
    'salary': [50000, 60000, 45000, 55000, 65000, 52000, 58000, 51000, 63000, 54000],
    'experience': [2, 8, 4, 3, 6, 4, 5, 3, 7, 2]
}
df = pd.DataFrame(data)
df
```

head나 tail은 지정한 갯수만큼 데이터를 미리 볼 수 있습니다. 지정하지 않으면 5개를 보여줍니다.

```python-exec
# head()로 첫 5행 보기
print("데이터 미리보기 (앞부분):")
df.head()
```

```python-exec
# tail()로 마지막 5행 보기
print("데이터 미리보기 (뒷부분):")
df.tail(3)  # 숫자를 지정하면 해당 행만큼 보여줌
```

info, describe 함수로 데이터의 기본 정보, 기본 통계 정보를 확인할 수 있습니다.

```python-exec
# info()로 데이터 기본 정보 확인
print("데이터 기본 정보:")
df.info()
```

```python-exec
# describe()로 기술 통계량 확인
print("기술 통계량:")
df.describe()
```

unique 함수로 고유값을 확인할 수 있습니다.

```python-exec
# 각 컬럼의 고유값 개수 확인
print("고유값 개수:")
df.nunique()
```

df.isnull().sum()으로 결측치 개수를 확인할 수 있습니다. 실무에서도 사용 빈도가 높은 함수입니다.

```python-exec
# 결측치 확인
print("결측치 개수:")
df.isnull().sum()
```

## 1.12 데이터 결합

데이터 분석시 여러 데이터를 결합해야 하는 경우가 많습니다. Pandas는 다양한 방식의 데이터 결합을 지원합니다.

### 1.12.1 concat으로 데이터 이어붙이기

```python-exec
import pandas as pd

# 첫 번째 데이터프레임
df1 = pd.DataFrame({
    'name': ['John', 'Anna', 'Peter'],
    'age': [25, 36, 29],
    'city': ['New York', 'Paris', 'Berlin']
})

# 두 번째 데이터프레임
df2 = pd.DataFrame({
    'name': ['Lisa', 'Tom', 'Sarah'],
    'age': [27, 32, 31],
    'city': ['London', 'Tokyo', 'Sydney']
})

df1 # df2 데이터도 확인해보세요.
```

concat 함수를 사용하여 데이터를 이어붙일 수 있습니다.

```python-exec
# 세로로 결합 (행 추가)
print("세로로 결합:")
pd.concat([df1, df2])
```

```python-exec
# 가로로 결합 (열 추가)
print("가로로 결합:")
pd.concat([df1, df2], axis=1)
```

### 1.12.2 merge로 데이터 병합하기

```python-exec
# 직원 정보 데이터프레임
employees = pd.DataFrame({
    'emp_id': [1, 2, 3, 4],
    'name': ['John', 'Anna', 'Peter', 'Linda'],
    'department': ['IT', 'HR', 'IT', 'HR']
})

# 급여 정보 데이터프레임
salaries = pd.DataFrame({
    'emp_id': [1, 2, 3],
    'salary': [50000, 60000, 45000],
    'bonus': [5000, 6000, 4500]
})

employees # salaries 데이터도 확인해보세요.
```

merge 함수를 사용하여 데이터를 병합할 수 있습니다. 보통 데이터를 병합하는 `JOIN`은 아래와 같이 4가지 종류가 있습니다. 이는 파이썬에서만 사용하는 방식이 아니라 SQL처럼 데이터를 다루는 대부분의 데이터에서 다루는 방식입니다.

![JOIN](/images/basecamp-sql/03-4-1.png)

이번 장에서는 `inner join`, `left join` 2개만 실습을 해보겠습니다. inner join은 두 데이터프레임의 공통된 키만을 가지고 병합하고, left join은 왼쪽 데이터프레임의 키를 모두 가지고 병합합니다. 2개의 코드를 실행해보고 결과를 비교해보세요.

```python-exec
# inner join (기본값)
print("Inner Join 결과:")
pd.merge(employees, salaries, on='emp_id')
```

```python-exec
# left join
print("Left Join 결과:")
pd.merge(employees, salaries, on='emp_id', how='left')
```

merge는 4개의 join을 지원합니다.

1. Inner Join (how='inner') - 기본값
2. Left Join (how='left')
3. Right Join (how='right')
4. Outer Join (how='outer')

### 1.12.3 join으로 데이터 결합하기

```python-exec
# 인덱스를 기준으로 결합
df1 = pd.DataFrame({
    'age': [25, 36, 29],
    'city': ['New York', 'Paris', 'Berlin']
}, index=['John', 'Anna', 'Peter'])

df2 = pd.DataFrame({
    'salary': [50000, 60000, 45000],
    'department': ['IT', 'HR', 'IT']
}, index=['John', 'Anna', 'Peter'])

print("Join 결과:")
df1.join(df2)
```

join으로 데이터를 결합할 때에는 인덱스를 기준으로 결합합니다. 이 때, 두 데이터프레임의 인덱스가 일치해야 합니다.

:::div{.callout}
**데이터 결합 방법 선택 기준**

- concat: 단순히 데이터를 이어붙일 때
- merge: 특정 컬럼을 기준으로 데이터를 결합할 때
- join: 인덱스를 기준으로 데이터를 결합할 때
  :::

각각의 결합 방법은 상황에 따라 장단점이 있으므로, 데이터의 구조와 결합 목적에 따라 적절한 방법을 선택해야 합니다.

## 1.13 데이터 정렬

네, Pandas의 데이터 정렬 파트를 작성하겠습니다.



## 1.13 데이터 정렬

Pandas에서는 `sort_values()`와 `sort_index()` 함수를 사용하여 데이터를 정렬할 수 있습니다. 단일 또는 여러 컬럼을 기준으로 오름차순이나 내림차순 정렬이 가능합니다. 옵션으로는 `ascending`을 사용하여 오름차순 또는 내림차순을 지정할 수 있습니다.

```python-exec
import pandas as pd

# 샘플 데이터 생성
data = {
    'name': ['John', 'Anna', 'Peter', 'Linda', 'Tom'],
    'age': [25, 36, 29, 24, 32],
    'salary': [50000, 60000, 45000, 55000, 65000]
}
df = pd.DataFrame(data)
df
```

```python-exec
# 나이를 기준으로 오름차순 정렬
print("나이 기준 오름차순:")
print(df.sort_values('age'))
```

```python-exec
# 급여를 기준으로 내림차순 정렬
print("급여 기준 내림차순:")
print(df.sort_values('salary', ascending=False))
```

```python-exec
# 여러 컬럼 기준으로 정렬
print("나이(오름차순)와 급여(내림차순) 기준:")
print(df.sort_values(['age', 'salary'], ascending=[True, False]))
```

```python-exec
# 인덱스 기준으로 정렬
print("인덱스 기준 정렬:")
print(df.sort_index())
```

group과 함께 사용할 수도 있습니다.

```python-exec
# 부서별 평균 연봉을 내림차순으로 정렬
data = {
    'name': ['John', 'Anna', 'Peter', 'Linda', 'Tom'],
    'department': ['IT', 'HR', 'IT', 'HR', 'IT'],
    'age': [25, 36, 29, 24, 32],
    'salary': [50000, 60000, 45000, 55000, 65000]
}
df = pd.DataFrame(data)
df
```

```python-exec
print("부서별 평균 연봉 내림차순 정렬:")
df.groupby('department')['salary'].mean().sort_values(ascending=False)
```

# 2. Pandas 실습

* Pandas의 기본 사용법을 익히고, 데이터를 분석해보는 실습을 진행합니다.

## 2.1 데이터 불러오기

```python-exec
import pandas as pd

# 온라인 쇼핑몰 주문 데이터
orders_data = {
    'order_id': range(1, 11),
    'customer_name': ['Kim', 'Lee', 'Park', 'Choi', 'Kim', 'Lee', 'Park', 'Choi', 'Kim', 'Lee'],
    'product': ['노트북', '스마트폰', '태블릿', '이어폰', '노트북', '스마트폰', '노트북', '태블릿', '이어폰', '노트북'],
    'quantity': [1, 2, 1, 3, 2, 1, 1, 2, 2, 1],
    'price': [1200000, 800000, 500000, 100000, 1200000, 800000, 1200000, 500000, 100000, 1200000],
    'order_date': ['2024-01-15', '2024-01-15', '2024-01-16', '2024-01-16', '2024-01-17', 
                   '2024-01-17', '2024-01-18', '2024-01-18', '2024-01-19', '2024-01-19']
}

df = pd.DataFrame(orders_data)
df
```

## 2.2 문제

### 2.2.1 기본 데이터 탐색 (20점)
1. 위 데이터를 DataFrame으로 만들고 처음 5행을 출력하세요. (5점)
```python-exec
# 여기에 코드를 작성하세요.
```

2. 데이터의 기본 정보(컬럼 타입, null 값 여부 등)를 확인하세요. (5점)
```python-exec
# 여기에 코드를 작성하세요.
```

3. 'order_date'를 datetime 타입으로 변환하세요. (5점)
```python-exec
# 여기에 코드를 작성하세요.
```

4. 각 컬럼의 기술통계량을 확인하세요. (5점)
```python-exec
# 여기에 코드를 작성하세요.
```

### 2.2.2 데이터 필터링 (30점)
1. 구매 수량(quantity)이 2개 이상인 주문을 조회하세요. (10점)
```python-exec
# 여기에 코드를 작성하세요.
```

2. 노트북을 구매한 고객들의 이름을 중복 없이 출력하세요. (10점)
```python-exec
# 여기에 코드를 작성하세요.
```

3. 총 구매액(quantity * price)이 1,000,000원 이상인 주문을 조회하세요. (10점)
```python-exec
# 여기에 코드를 작성하세요.
```

### 2.2.3 데이터 그룹화와 집계 (30점)
1. 고객별 총 구매액을 계산하고 내림차순으로 정렬하세요. (10점)
```python-exec
# 여기에 코드를 작성하세요.
```

2. 제품별 평균 구매 수량을 계산하세요. (10점)
```python-exec
# 여기에 코드를 작성하세요.
```

3. 날짜별 총 매출액을 계산하고, 매출액이 가장 높은 날짜를 찾으세요. (10점)
```python-exec
# 여기에 코드를 작성하세요.
```

### 2.2.4 데이터 변환 (20점)
1. 총 구매액을 계산하여 'total_amount'라는 새로운 컬럼을 만드세요. (10점)
```python-exec
# 여기에 코드를 작성하세요.
```

2. 구매액에 따라 다음과 같이 고객 등급을 부여하는 'customer_grade' 컬럼을 만드세요: (10점)
   - 200만원 이상: 'VIP'
   - 100만원 이상: 'Gold'
   - 50만원 이상: 'Silver'
   - 50만원 미만: 'Bronze'
```python-exec
# 여기에 코드를 작성하세요.
```
