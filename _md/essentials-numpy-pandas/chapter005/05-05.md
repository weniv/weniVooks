---
chapter: Pandas :다섯 번째 걸음
title: Pandas의 다양한 예제 
date: 2024-07-25
---



# 1. 차원 축소 
차원 축소(Dimensionality Reduction)는 데이터에서 중요한 정보를 최대한 유지하면서 차원을 줄이는 과정으로, 이는 데이터의 효율적인 분석과 시각화, 계산 복잡도의 감소를 목적으로 사용됩니다. PCA(주성분 분석)는 이러한 차원 축소의 대표적인 기법으로, 데이터의 분산을 최대한 보존하는 새로운 축(주성분)을 찾아 데이터를 축소합니다. 각 주성분은 기존 데이터의 선형 결합으로 구성되며, 가장 큰 분산을 가지는 방향부터 순서대로 결정됩니다.

```python-exec
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

# 가상의 데이터프레임 생성 (100차원, 1000개의 샘플)
df = pd.DataFrame(np.random.rand(1000, 100), columns=[f'feature_{i}' for i in range(100)])

# PCA를 사용하여 10차원으로 축소
pca = PCA(n_components=10)
df_reduced = pca.fit_transform(df)

# 결과를 다시 데이터프레임으로 변환
df_reduced = pd.DataFrame(df_reduced, columns=[f'PC{i+1}' for i in range(10)])
print(df_reduced.head())
```
|    |      PC1 |      PC2 |      PC3 |      PC4 |      PC5 |      PC6 |      PC7 |      PC8 |      PC9 |     PC10 |
|---:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|
|  0 |  0.98937 | -0.70133 | -0.98220 |  0.54244 |  0.09221 | -0.63098 |  0.01320 |  0.42784 | -0.13966 | -0.64672 |
|  1 | -1.24687 | -0.34241 |  0.57866 | -0.67229 | -0.40573 |  0.07359 | -0.25450 | -0.05623 |  0.61322 |  0.42345 |
|  2 | -0.07827 |  0.21248 |  0.27274 |  0.42134 |  0.04123 | -0.60457 | -0.24098 |  0.00898 |  0.33149 |  0.25967 |
|  3 |  1.28871 | -0.04695 |  0.31484 |  0.50132 | -0.33293 |  0.45708 |  0.29185 |  0.45630 |  0.36122 |  0.06283 |
|  4 | -1.02401 | -0.72736 | -0.11184 |  0.64395 | -0.15325 |  0.04281 |  0.62440 | -0.16036 | -0.41202 | -0.51399 |



# 2. 군집화 
군집화(Clustering)는 데이터를 유사한 특성을 가진 그룹(군집)으로 나누는 비지도 학습 기법으로, 이를 통해 데이터 간의 구조와 패턴을 발견할 수 있습니다. K-means 군집화는 대표적인 군집화 알고리즘으로, 데이터를 K개의 군집으로 나눕니다. 이 알고리즘은 먼저 초기 중심점(centroid)을 설정한 후, 각 데이터 포인트를 가장 가까운 중심점에 할당하고, 중심점을 재계산하는 과정을 반복합니다.

```python-exec
import pandas as pd
from sklearn.cluster import KMeans

# 가상의 고객 데이터프레임 생성
data = {'구매금액': [100, 200, 50, 300, 150], '방문빈도': [2, 5, 1, 7, 3]}
df = pd.DataFrame(data)

# K-means 군집화 (3개의 군집으로 나눔)
kmeans = KMeans(n_clusters=3)
df['군집'] = kmeans.fit_predict(df[['구매금액', '방문빈도']])

print(df)
```
|    |   구매금액 |   방문빈도 |   군집 |
|---:|----------:|----------:|-------:|
|  0 |       100 |         2 |      2 |
|  1 |       200 |         5 |      0 |
|  2 |        50 |         1 |      2 |
|  3 |       300 |         7 |      1 |
|  4 |       150 |         3 |      0 |

# 3. 상관관계 분석 

상관관계 분석(Correlation Analysis)은 두 변수 간의 관계를 정량적으로 평가하는 방법입니다. 피어슨 상관계수는 두 변수 간의 선형 관계를 나타내며, 이 값은 -1에서 1 사이의 범위를 가집니다. 피어슨 상관계수(Pearson Correlation Coefficient)는 두 변수 간의 선형 상관성을 측정하는 통계적 지표로, 1에 가까울수록 강한 양의 상관관계를, -1에 가까울수록 강한 음의 상관관계를 의미하며, 0에 가까울수록 상관관계가 없음을 나타냅니다.
이 실습에서는 학생들의 공부 시간과 시험 점수 간의 상관관계를 피어슨 상관계수를 통해 분석합니다. 이를 통해 두 변수 간의 연관성을 쉽게 파악할 수 있습니다.

```python-exec
import pandas as pd

# 가상의 데이터프레임 생성
data = {'공부시간': [2, 3, 5, 4, 6, 1, 3], '시험점수': [60, 70, 85, 75, 90, 55, 65]}
df = pd.DataFrame(data)

# 피어슨 상관계수 계산
correlation = df.corr(method='pearson')
print(correlation)
```


# 4. 텍스트 데이터 처리 

텍스트 데이터 처리(Text Data Processing)는 텍스트 데이터를 분석하기 위해 전처리하고, 텍스트에서 유의미한 정보를 추출하는 과정을 포함합니다. TF-IDF(Term Frequency-Inverse Document Frequency)는 문서에서 단어의 중요도를 계산하는 방법으로, 단어의 빈도(TF)와 역문서 빈도(IDF)를 곱하여 특정 단어가 문서 내에서 얼마나 중요한지를 나타냅니다.

이 실습에서는 뉴스 기사에 대한 TF-IDF 값을 계산하여, 각 문서에서 단어들이 얼마나 중요한지를 평가합니다. TF-IDF는 정보 검색과 텍스트 마이닝 등에서 널리 사용되는 기법입니다.

```python-exec
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# 샘플 뉴스 기사 데이터프레임 생성
df = pd.DataFrame({
    '기사': [
        "The quick brown fox jumps over the lazy dog",
        "The lazy dog sleeps all day",
        "The quick brown fox is very clever"
    ]
})

# TF-IDF 벡터화
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['기사'])

# TF-IDF 값을 데이터프레임으로 변환
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())
print(df_tfidf)
```


