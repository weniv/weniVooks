---
chapter: Pandas 활용
title: Pandas의 다양한 예제 
date: 2024-07-25
---

# 1. 차원 축소 

차원 축소(Dimensionality Reduction)는 데이터에서 중요한 정보를 최대한 유지하면서 데이터의 차원을 줄이는 과정을 의미합니다. 이 방법은 데이터를 더 효율적으로 분석하고 시각화할 수 있게 해주며, 계산 복잡도를 줄이는 데도 도움을 줍니다. 대표적인 차원 축소 기법 중 하나인 PCA(주성분 분석)는 데이터의 분산을 최대한 보존하는 새로운 축, 즉 주성분을 찾아 데이터를 축소합니다. 각 주성분은 기존 데이터의 선형 결합으로 구성되며, 데이터의 분산이 가장 큰 방향을 기준으로 차례대로 결정됩니다.
```python-exec
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA

# 가상의 데이터프레임 생성 (100차원, 1000개의 샘플)
df = pd.DataFrame(np.random.rand(1000, 100), columns=[f'feature_{i}' for i in range(100)])

# PCA를 사용하여 10차원으로 축소
pca = PCA(n_components=10)
df_reduced = pca.fit_transform(df)

# 결과를 다시 데이터프레임으로 변환
df_reduced = pd.DataFrame(df_reduced, columns=[f'PC{i+1}' for i in range(10)])
print(df_reduced.head())
```
|    |      PC1 |      PC2 |      PC3 |      PC4 |      PC5 |      PC6 |      PC7 |      PC8 |      PC9 |     PC10 |
|---:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|
|  0 |  0.98937 | -0.70133 | -0.98220 |  0.54244 |  0.09221 | -0.63098 |  0.01320 |  0.42784 | -0.13966 | -0.64672 |
|  1 | -1.24687 | -0.34241 |  0.57866 | -0.67229 | -0.40573 |  0.07359 | -0.25450 | -0.05623 |  0.61322 |  0.42345 |
|  2 | -0.07827 |  0.21248 |  0.27274 |  0.42134 |  0.04123 | -0.60457 | -0.24098 |  0.00898 |  0.33149 |  0.25967 |
|  3 |  1.28871 | -0.04695 |  0.31484 |  0.50132 | -0.33293 |  0.45708 |  0.29185 |  0.45630 |  0.36122 |  0.06283 |
|  4 | -1.02401 | -0.72736 | -0.11184 |  0.64395 | -0.15325 |  0.04281 |  0.62440 | -0.16036 | -0.41202 | -0.51399 |



# 2. 군집화 
군집화(Clustering)는 데이터를 유사한 특성을 가진 그룹으로 나누는 비지도 학습 기법으로, 이를 통해 데이터 내의 구조와 패턴을 파악할 수 있습니다. 대표적인 군집화 알고리즘인 K-means는 데이터를 K개의 군집으로 나누는 방법입니다. 이 알고리즘은 먼저 초기 중심점(centroid)을 설정하고, 각 데이터 포인트를 가장 가까운 중심점에 할당한 후, 군집의 중심점을 재계산하는 과정을 반복하면서 최적의 군집을 찾아냅니다.

```python-exec
import pandas as pd
from sklearn.cluster import KMeans

# 가상의 고객 데이터프레임 생성
data = {'구매금액': [100, 200, 50, 300, 150], '방문빈도': [2, 5, 1, 7, 3]}
df = pd.DataFrame(data)

# K-means 군집화 (3개의 군집으로 나눔)
kmeans = KMeans(n_clusters=3)
df['군집'] = kmeans.fit_predict(df[['구매금액', '방문빈도']])

print(df)
```
|    |   구매금액 |   방문빈도 |   군집 |
|---:|----------:|----------:|-------:|
|  0 |       100 |         2 |      2 |
|  1 |       200 |         5 |      0 |
|  2 |        50 |         1 |      2 |
|  3 |       300 |         7 |      1 |
|  4 |       150 |         3 |      0 |

# 3. 상관관계 분석 

상관관계 분석(Correlation Analysis)은 두 변수 간의 관계를 정량적으로 평가하는 방법으로, 피어슨 상관계수는 두 변수 간의 선형 관계를 나타내는 대표적인 지표입니다. 피어슨 상관계수의 값은 -1에서 1 사이로, 1에 가까울수록 강한 양의 상관관계를, -1에 가까울수록 강한 음의 상관관계를 의미하며, 0에 가까울수록 두 변수 간의 상관관계가 거의 없음을 나타냅니다. 이번 실습에서는 학생들의 공부 시간과 시험 점수 간의 상관관계를 피어슨 상관계수를 통해 분석하여, 두 변수 사이의 연관성을 직관적으로 파악하고 이해하는 과정을 다룹니다.

```python-exec
import pandas as pd

# 가상의 데이터프레임 생성
data = {'공부시간': [2, 3, 5, 4, 6, 1, 3], '시험점수': [60, 70, 85, 75, 90, 55, 65]}
df = pd.DataFrame(data)

# 피어슨 상관계수 계산
correlation = df.corr(method='pearson')
print(correlation)
```


# 4. 텍스트 데이터 처리 
텍스트 데이터 처리(Text Data Processing)는 텍스트 데이터를 분석하기 위해 전처리하고 유의미한 정보를 추출하는 과정을 포함하며, TF-IDF(Term Frequency-Inverse Document Frequency)는 문서 내 단어의 중요도를 계산하는 대표적인 방법입니다. 이 방법은 특정 단어의 빈도(TF)와 역문서 빈도(IDF)를 곱해 해당 단어가 문서에서 얼마나 중요한지를 나타냅니다. 이번 실습에서는 뉴스 기사에 대해 TF-IDF 값을 계산해 각 문서에서 단어의 중요성을 평가하며, TF-IDF는 정보 검색과 텍스트 마이닝에서 널리 활용되는 기법입니다.

```python-exec
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# 샘플 뉴스 기사 데이터프레임 생성
df = pd.DataFrame({
    '기사': [
        "The quick brown fox jumps over the lazy dog",
        "The lazy dog sleeps all day",
        "The quick brown fox is very clever"
    ]
})

# TF-IDF 벡터화
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df['기사'])

# TF-IDF 값을 데이터프레임으로 변환
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())
print(df_tfidf)
```


